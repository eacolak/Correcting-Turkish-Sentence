# Bu kodlar Transformer mimarisi yapmaya çalıştığım kısımdır
# Fasttext bu mimari içi kullanılmadı çünkü elimizdeki karakter sayısı zaten az
# fasttext gibi büyük bir dosyayı koda entegre etmemize gerek yok

# Bu kodda elimizdeki transformer modelini Pre-LN modeline çeviriyoruz. Buradaki değişiklik transformer modellerinde yaptığımız normalizasyonu öne çekmektir
# başka bir değişiklik yapmıyoruz. Bu model sonucunda elimize şu şekil bir val loss verdi:

#Epoch 45 Batch 0 Loss 0.0024 Gradient Norm: 0.0622
#Epoch 45 Batch 50 Loss 0.0023 Gradient Norm: 0.1184
#Epoch 45 Batch 100 Loss 0.0013 Gradient Norm: 0.0449
#Epoch 45 Batch 150 Loss 0.0008 Gradient Norm: 0.0450
#Epoch 45 Batch 200 Loss 0.0011 Gradient Norm: 0.0430
#Epoch 45 -> Train Loss 0.0013 | Validation Loss 0.0025
#Time taken for 1 epoch: 97.29 secs
#Validation loss iyileşti (0.0025 --> 0.0025). En iyi model kaydediliyor...

# Fakat test aşamasında elimizdeki modelin düzeltmesi saçmaladı. Bu yüzden birdahaki adımımda Hazır transformers modelleriyle çalışacağım.
# Herkez olarak verilen kelimeyi bu şekilde bir düzeltmeye gitti. Saçmaladı. Modelimiz hala collapse döngüsünden çıkamıyor.
#--- Adım 150 ---
#Modelin Gördüğü Girdi: '<e<ee<eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeteteteteteteteteteteeee'
#En olası 5 tahmin:
#  -> 'e' (Olasılık: 97.87%)
#  -> '<' (Olasılık: 1.32%)
#  -> 't' (Olasılık: 0.66%)
#  -> ' ' (Olasılık: 0.07%)
#  -> 'r' (Olasılık: 0.02%)




import time
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# --- Google Colab için Drive Bağlantısı ---
from google.colab import drive
drive.mount('/content/drive')

# ===================================================================
# BÖLÜM 1: KONFİGÜRASYON VE HİPERPARAMETRELER
# ===================================================================
# Tüm ayarlarımızı buradan, tek bir yerden yöneteceğiz.
# Deneme yapmak istediğinde SADECE BU BÖLÜMÜ DEĞİŞTİR.

# --- Dosya Yolları ---
PROJE_KLASORU = '/content/drive/MyDrive/TransformerModelDataset/'

# --- Veri ve Tokenizer Ayarları ---
MAX_SEQ_LENGTH = 150
BATCH_SIZE = 64

# --- Model Mimarisi Ayarları ---
NUM_LAYERS = 4
D_MODEL = 256
DFF = 1024
NUM_HEADS = 8
DROPOUT_RATE = 0.1

# --- Eğitim Ayarları ---
EPOCHS = 50
LEARNING_RATE = 5e-5 # 0.00005
USE_GRADIENT_CLIPPING = True

# YENİ: Erken Durdurma Ayarı
# Validation loss kaç epoch boyunca iyileşmezse eğitimin duracağını belirler.
EARLY_STOPPING_PATIENCE = 5

# ===================================================================
# BÖLÜM 2: VERİ YÜKLEME VE İŞLEME
# ===================================================================

json_dosya_yolu = os.path.join(PROJE_KLASORU, 'yazim_yanlislari_dataset.json')
print(f"Drive'dan json dosyası okunuyor: {json_dosya_yolu}")
try:
    df = pd.read_json(json_dosya_yolu)
    hatali_metinler = df['hatali_metin'].tolist()
    dogru_metinler = df['dogru_metin'].tolist()
except FileNotFoundError:
    print(f"HATA: Dosya bulunamadı! Lütfen '{json_dosya_yolu}' dosyasının doğru yerde olduğundan emin olun.")
    exit()
except Exception as e:
    print(f"Hata:{e}")
    exit()

train_h, test_val_h, train_d, test_val_d = train_test_split(
    hatali_metinler, dogru_metinler, test_size=0.2, random_state=42
)
val_h, test_h, val_d, test_d = train_test_split(
    test_val_h, test_val_d, test_size=0.5, random_state = 42
)
print(f"Eğitim seti boyutu: {len(train_h)}")
print(f"Doğrulama seti boyutu: {len(val_h)}")
print(f"Test seti boyutu: {len(test_h)}")

train_input_texts = ["<start>" + metin.lower().strip() + "<end>" for metin in train_h]
train_target_texts = ["<start>" + metin.lower().strip() + "<end>" for metin in train_d]
val_input_texts = ["<start>" + metin.lower().strip() + "<end>" for metin in val_h]
val_target_texts = ["<start>" + metin.lower().strip() + "<end>" for metin in val_d]
test_input_texts = ["<start>" + metin.lower().strip() + "<end>" for metin in test_h]
test_target_texts = ["<start>" + metin.lower().strip() + "<end>" for metin in test_d]

input_vectorizer = tf.keras.layers.TextVectorization(
    output_sequence_length = MAX_SEQ_LENGTH,
    standardize = None,
    split = 'character',
)
target_vectorizer = tf.keras.layers.TextVectorization(
    output_sequence_length = MAX_SEQ_LENGTH + 1,
    standardize = None,
    split = 'character',
)
input_vectorizer.adapt(train_input_texts)
target_vectorizer.adapt(train_target_texts)

train_input_seq = input_vectorizer(np.array(train_input_texts))
train_target_seq = target_vectorizer(np.array(train_target_texts))
val_input_seq = input_vectorizer(np.array(val_input_texts))
val_target_seq = target_vectorizer(np.array(val_target_texts))

def create_dataset(input_seq, target_seq):
    encoder_input = input_seq
    decoder_input = target_seq[:, :-1]
    decoder_target = target_seq[:, 1:]
    return tf.data.Dataset.from_tensor_slices(((encoder_input, decoder_input),decoder_target))

train_dataset = create_dataset(train_input_seq, train_target_seq)
val_dataset = create_dataset(val_input_seq, val_target_seq)

BUFFER_SIZE =len(train_input_seq)
train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

print("Eğitim ve Doğrulama datasetleri oluşturuldu.")

# ===================================================================
# BÖLÜM 3: MODEL MİMARİSİ TANIMLARI
# ===================================================================

def create_look_ahead_mask(size):
    mask = 1-tf.linalg.band_part(tf.ones((size,size)), -1, 0)
    return mask

class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, position, d_model):
        super(PositionalEncoding ,self).__init__()
        self.pos_encoding = self.positional_encoding(position, d_model)
    def get_angles(self, position, i, d_model):
        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
        return position * angle_rates
    def positional_encoding(self, position, d_model):
        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)
        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
        pos_encoding = angle_rads[np.newaxis, ...]
        return tf.cast(pos_encoding, dtype=tf.float32)
    def call(self, inputs):
        seq_len = tf.shape(inputs)[1]
        return inputs + self.pos_encoding[:, :seq_len, :]

class MultiHeadSelfAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadSelfAttention, self).__init__()
        self.attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = d_model // num_heads)
        self.dense_layer = tf.keras.layers.Dense(d_model)
    def call(self, inputs):
        attention_output = self.attention(query = inputs, value = inputs, key = inputs)
        return self.dense_layer(attention_output)

class PositionWiseFeedForwardNetwork(tf.keras.layers.Layer):
    def __init__(self, d_model, dff):
        super(PositionWiseFeedForwardNetwork, self).__init__()
        self.dense1 = tf.keras.layers.Dense(dff, activation = 'relu')
        self.dense2 = tf.keras.layers.Dense(d_model)
    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)

# PRE-LAYER NORMALIZATION MİMARİSİ
class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):
        super(EncoderLayer, self).__init__()
        self.mha = MultiHeadSelfAttention(d_model=d_model, num_heads=num_heads)
        self.ffn = PositionWiseFeedForwardNetwork(d_model, dff=dff)
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)

    def call(self, inputs, training):
        norm_inputs = self.layernorm1(inputs)
        attn_output = self.mha(norm_inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = inputs + attn_output

        norm_out1 = self.layernorm2(out1)
        ffn_output = self.ffn(norm_out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = out1 + ffn_output
        return out2

class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 maximum_position_encoding, dropout_rate = 0.1):
        super(Encoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(maximum_position_encoding, self.d_model)
        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
    def call(self, inputs, training):
        x = self.embedding(inputs)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x = self.pos_encoding(x)
        x = self.dropout(x, training = training)
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training=training)
        return x

class CausalSelfAttention (tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(CausalSelfAttention, self).__init__()
        self.attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads,key_dim = d_model // num_heads)
        self.dense_layer = tf.keras.layers.Dense(d_model)
    def call(self, inputs, mask):
        attention_output = self.attention(query = inputs, value = inputs, key = inputs, attention_mask = mask)
        return self.dense_layer(attention_output)

class CrossAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(CrossAttention, self).__init__()
        self.attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = d_model // num_heads)
        self.dense_layer = tf.keras.layers.Dense(d_model)
    def call(self, inputs, context):
        attention_output = self.attention(query = inputs, key = context, value = context)
        return self.dense_layer(attention_output)

# PRE-LAYER NORMALIZATION MİMARİSİ
class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):
        super(DecoderLayer, self).__init__()
        self.causal_self_attention = CausalSelfAttention(d_model, num_heads)
        self.cross_attention = CrossAttention(d_model, num_heads)
        self.ffn = PositionWiseFeedForwardNetwork(d_model, dff)
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)

    def call(self, inputs, context, training):
        seq_len = tf.shape(inputs)[1]
        look_ahead_mask = create_look_ahead_mask(seq_len)

        norm_inputs = self.layernorm1(inputs)
        attn1 = self.causal_self_attention(norm_inputs, mask=look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = inputs + attn1

        norm_out1 = self.layernorm2(out1)
        attn2 = self.cross_attention(norm_out1, context)
        attn2 = self.dropout2(attn2, training=training)
        out2 = out1 + attn2

        norm_out2 = self.layernorm3(out2)
        ffn_output = self.ffn(norm_out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = out2 + ffn_output
        return out3

class Decoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, dropout_rate=0.1):
        super(Decoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)
        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
    def call(self, inputs, context, training):
        x = self.embedding(inputs)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x = self.pos_encoding(x)
        x = self.dropout(x, training=training)
        for i in range(self.num_layers):
            x = self.dec_layers[i](x, context, training=training)
        return x

class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 target_vocab_size, max_pos_input, max_pos_target, dropout_rate = 0.1):
        super(Transformer, self).__init__()
        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, max_pos_input, dropout_rate)
        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, max_pos_target, dropout_rate)
        self.final_layer = tf.keras.layers.Dense(target_vocab_size)
    def call(self,inputs,training):
        enc_input, dec_input = inputs
        enc_output = self.encoder(enc_input, training=training)
        dec_output = self.decoder(dec_input, enc_output, training=training)
        final_output = self.final_layer(dec_output)
        return final_output

# ===================================================================
# BÖLÜM 4: EĞİTİMİN HAZIRLANMASI
# ===================================================================

print("\nModel ve Eğitim Ayarları Yapılıyor...")

input_vocab_size = len(input_vectorizer.get_vocabulary())
target_vocab_size = len(target_vectorizer.get_vocabulary())

transformer = Transformer(
    num_layers=NUM_LAYERS,
    d_model=D_MODEL,
    num_heads=NUM_HEADS,
    dff=DFF,
    input_vocab_size=input_vocab_size,
    target_vocab_size=target_vocab_size,
    max_pos_input=MAX_SEQ_LENGTH,
    max_pos_target=MAX_SEQ_LENGTH + 1,
    dropout_rate=DROPOUT_RATE
)

if USE_GRADIENT_CLIPPING:
    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=1.0)
    print(f"Optimizer: Adam with Gradient Clipping (clipnorm=1.0) and LR={LEARNING_RATE}")
else:
    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)
    print(f"Optimizer: Adam without Gradient Clipping and LR={LEARNING_RATE}")

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none'
)

def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)

@tf.function
def train_step(inputs, target):
    encoder_input, dec_input = inputs
    real_target = target
    with tf.GradientTape() as tape:
        predictions = transformer((encoder_input, dec_input), training=True)
        loss = loss_function(real_target, predictions)
    gradients = tape.gradient(loss, transformer.trainable_variables)

    global_norm = tf.linalg.global_norm(gradients)

    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    return loss, global_norm

@tf.function
def val_step(inputs, target):
    encoder_input, dec_input = inputs
    real_target = target
    predictions = transformer((encoder_input, dec_input), training=False)
    loss = loss_function(real_target, predictions)
    return loss

# ===================================================================
# BÖLÜM 5: EĞİTİM DÖNGÜSÜ (ERKEN DURDURMA VE CHECKPOINT İLE GÜNCELLENDİ)
# ===================================================================

print(f"\n--- Eğitim Döngüsü Başlıyor ({EPOCHS} Epoch) ---")

# --- YENİ: Erken Durdurma ve Checkpoint için Değişkenler ---
best_val_loss = float('inf')
patience_counter = 0
agirliklar_kayit_yolu = os.path.join(PROJE_KLASORU, 'transformer_yazim_duzeltme_en_iyi.weights.h5') # En iyi modeli farklı bir isimle kaydedelim
# --- Bitti ---

gradient_norms = []
for epoch in range(EPOCHS):
    start = time.time()
    total_train_loss = 0
    total_val_loss = 0

    for(batch, (inputs, target)) in enumerate(train_dataset):
        batch_loss, norm = train_step(inputs, target)
        gradient_norms.append(norm.numpy())
        total_train_loss += batch_loss

        if batch % 50 == 0:
            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f} Gradient Norm: {norm.numpy():.4f}')

    for (batch, (inputs, target)) in enumerate(val_dataset):
        batch_loss = val_step(inputs, target)
        total_val_loss += batch_loss

    avg_train_loss = total_train_loss / len(train_dataset)
    avg_val_loss = total_val_loss / len(val_dataset)

    print(f'Epoch {epoch + 1} -> Train Loss {avg_train_loss:.4f} | Validation Loss {avg_val_loss:.4f}')
    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs')

    # --- YENİ: Her Epoch Sonunda Erken Durdurma ve Checkpoint Kontrolü ---
    if avg_val_loss < best_val_loss:
        print(f'Validation loss iyileşti ({best_val_loss:.4f} --> {avg_val_loss:.4f}). En iyi model kaydediliyor...')
        best_val_loss = avg_val_loss
        transformer.save_weights(agirliklar_kayit_yolu) # Sadece en iyi modeli kaydet
        patience_counter = 0  # Sabrı sıfırla
    else:
        patience_counter += 1
        print(f'Validation loss iyileşmedi. En iyi loss: {best_val_loss:.4f}. Sabır: {patience_counter}/{EARLY_STOPPING_PATIENCE}')

    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"\n{EARLY_STOPPING_PATIENCE} epoch boyunca validation loss iyileşmedi. Erken durdurma tetiklendi!")
        break  # Eğitim döngüsünü sonlandır
    print("-" * 50)


# ===================================================================
# BÖLÜM 6: SONUÇLARI GÖRSELLEŞTİRME VE KAYDETME
# ===================================================================

print("\nEğitim tamamlandı. Sonuçlar çizdiriliyor...")

# Gradyan Norm Grafiği
plt.figure(figsize=(12, 6))
plt.plot(gradient_norms)
plt.title('Eğitim Sırasındaki Gradyan Normu Değişimi')
plt.xlabel('Eğitim Adımları (Steps)')
plt.ylabel('Global Gradyan Normu')
plt.grid(True)
plt.show()

print("En iyi modelin ağırlıkları eğitim sırasında 'transformer_yazim_duzeltme_en_iyi.weights.h5' olarak kaydedildi.")